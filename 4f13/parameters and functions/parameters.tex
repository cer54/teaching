\input{../style.tex}

\title{Distributions over parameters and functions}
\author{Carl Edward Rasmussen}
\date{July 1st, 2016}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Key concepts}

\end{frame}


\begin{frame}
\frametitle{Multiple explanations of the data}

\vspace{-0.5ex}
\begin{itemize}
\item We do not \Blue{believe} all models are equally probable to explain the data.
\item We may \Blue{believe} a simpler model is more probable than a complex one.
\end{itemize}
%
Model complexity and uncertainty:
\vspace{-0.5ex}
\begin{itemize}
\item We do not \Blue{know} what particular function generated the data.
\item More than one of our models can perfectly fit the data.
\item We \Blue{believe} more than one of our models could have generated the data.
\item We want to reason in terms of \Red{a set of possible explanations}, not just one.
\end{itemize}
%
\centerline{
\includegraphics[width=0.45\textwidth]{random_polynomials_degree17.pdf}
\includegraphics[width=0.45\textwidth]{samples_posterior_degree17.pdf}
}
%
\end{frame}

\begin{frame}
\frametitle{Posterior probability of a function}

Given the \Blue{prior}  functions \Blue{$p(\bff)$} how can we make predictions?
\begin{itemize}
\item Of all functions generated from the prior, keep those that fit the data.
\item The notion of closeness to the data is given by the \Red{likelihood $p(\bfy|\bff)$}.
\item We are really interested in the posterior distribution over functions:
%
\[
\Green{p(\bff|\bfy)}\;=\;\frac{\Red{p(\bfy|\bff)}\,\Blue{p(\bff)}}{\Cyan{p(\bfy)}}
{\; \; \; \; \; \;} \mathrm{Bayes\; Rule}
\]
%
\end{itemize}

\parbox{0.45\textwidth}{
\centerline{\includegraphics[width=0.45\textwidth]{random_polynomials_degree17.pdf}}
\centerline{Some samples from the prior}
}
\parbox{0.45\textwidth}{
\centerline{\includegraphics[width=0.45\textwidth]{samples_posterior_degree17.pdf}}
\centerline{Samples from the posterior}
}
\end{frame}


\begin{frame}
\frametitle{Priors on parameters induce priors on functions}

A model $\mathcal{M}$ is the choice of a \Red{model structure} and of \Blue{parameter values}.  
%
\[
f_\bfw(x)\; =\; \sum_{m=0}^{\Red{M}} \Blue{w_m}\,\Red{\phi_m(x)}
\]
%
The prior $p(\bfw|\mathcal{M})$ determines what \Blue{functions} this model can generate. Example: \vspace*{-1.5ex}
\begin{itemize}
\item Imagine we choose $M=17$, and $p(w_m)=\N(w_m;\;0, \sigma_\bfw^2)$.
\item We have actually defined a \Blue{prior distribution over functions $p(\bff|\mathcal{M})$}.
\end{itemize} \vspace*{-1.5ex}

\parbox{0.49\textwidth}{
This figure is generated as follows:
\begin{itemize}
\item Use polynomial basis functions, $\phi_m(x)=x^m$.
\item Define a uniform grid of $n=100$ values in $x$ from $[-1.5, 2]$.
\item Generate matrix $\bPhi$ for $M=17$.
%\texttt{Phi = repmat(x,1,M+1).\^{}repmat(0:M,N,1);}
\item Draw $w_m\sim{\cal N}(0,1)$.
\item Compute and plot $\bff=\bPhi_{n\times 18}\,\bfw$.
\end{itemize}
}
\parbox{0.45\textwidth}{
\centerline{\includegraphics[width=0.5\textwidth]{random_polynomials_degree17.pdf}}
}

\end{frame}
\end{document}