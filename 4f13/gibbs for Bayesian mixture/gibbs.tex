\input{../style.tex}

\title{Gibbs Sampling for Bayesian Mixture}
\author{Carl Edward Rasmussen}
\date{November 26th, 2017}

\begin{document}


\begin{frame}
\titlepage
\end{frame}


\begin{frame}
\frametitle{Key concepts}

\begin{itemize}
\item General Bayesian mixture model
\item We derive the Gibbs sampler
\item Marginalize out mixing proportions: collapsed Gibbs sampler
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Bayesian document mixture model}

Our mixture model has observations ${\bf w}_d$ the words in document
$d=1,\ldots,D$. The parameters are $\boldsymbol\beta_k$ and $\boldsymbol\theta$, and latent variables ${\bf z}$.\\[1ex]

The mixture model has $K$ components, so the parameters are
$\boldsymbol\beta_k, k=1,\ldots K$. Each $\boldsymbol\beta_k$ is the
parameter of a categorical over possible words, with prior
$p(\boldsymbol\beta)$. The discrete latent variables
$z_d, d=1,\ldots D$ take on values $1,\dots
K$.\\[1ex]

Note, that in this model the observations are (the word counts of)
entire documents.

\begin{minipage}{0.7\linewidth}
\centerline{\includegraphics[width=0.9\linewidth]{bayes_mix_categorical_model}}
\end{minipage}
\begin{minipage}{0.29\linewidth}
{\small
\begin{eqnarray*}
\btheta & \sim & \mathrm{Dir}(\alpha) \\
\bbeta_k & \sim & \mathrm{Dir}(\gamma) \\
z_d | \btheta & \sim & \rm{Cat}(\btheta)\\
w_{nd} | z_d, \bbeta & \sim & \rm{Cat}(\bbeta_{z_d})
\end{eqnarray*}
}
\end{minipage}
\end{frame}



\begin{frame}
\frametitle{Bayesian mixture model}

The conditional likelihood is for each observation is
\[
p({\bf w}_d|z_d=k,\boldsymbol\beta)\;=\;p({\bf
  w}_d|\beta_k)\;=\;p({\bf w}_d|\beta_{z_d}),
\]
and the prior
\[
p(\boldsymbol\beta_k).
\]

The categorical latent component assignment probability
\[
p(z_d=k|\boldsymbol\theta)\;=\;\theta_k,
\]
with a Dirichlet prior
\[
p(\theta|\alpha)\;=\;{\rm Dir}(\alpha).
\]

Therefore, the latent conditional posterior is
\[
p(z_d=k|{\bf w}_d,\boldsymbol\theta,\boldsymbol\beta)\;\propto
p(z_d=k|\boldsymbol\theta)p({\bf w}_d|z_d=k,\boldsymbol\beta)\;\propto\;
\theta_k p({\bf w}_d|\beta_{z_d}),
\]
which is just a discrete distribution with $K$ possible outcomes.
\end{frame}


\begin{frame}
\frametitle{Gibbs Sampling}

Iteratively, alternately, sample the three types of variables:\\[1ex]

Component parameters
\[
p(\beta_k|{\bf w},{\bf z})\;\propto\;p(\beta_k)\prod_{d:z_d=k}p({\bf w}_d|\beta_k),
\]
which is now a categorical model, the mixture aspect having been eliminated.\\[1ex]

The posterior latent conditional allocations
\[
p(z_d=k|{\bf w}_d,\boldsymbol\theta,\boldsymbol\beta)\;\propto \theta_k
p({\bf w}_d|\beta_{z_d}),
\]
are categorical and mixing proportions
\[
p(\theta|{\bf z},\alpha)\;\propto\;p(\theta|\alpha)p({\bf z}|\theta)\;=\;
{\rm Dir}(\frac{c_k+\alpha_k}{\sum_{j=1}^K c_j+\alpha_j}).
\]
where $c_k=\sum_{d:z_d=k}1$ are the counts for mixture $k$.
\end{frame}

\begin{frame}
\frametitle{Collapsed Gibbs Sampler}

The parameters are treated in the same way as before.\\[1ex]

If we \Blue{marginalize} over $\theta$
\[
p(z_d=k|{\bf z}_{-d},\alpha)\;=\;\frac{\alpha+c_{-d,k}}{\sum_{j=1}^K\alpha+c_{-d,j}},
\]
where index $-d$ means \emph{all except} $d$, and $c_k$ are counts;\\
we derived this result when discussing pseudo counts.\\[1ex]

The \Blue{collapsed} Gibbs sampler for the latent assignements
\[
p(z_d=k|{\bf w}_d,z_{-d},\boldsymbol\beta,\alpha)\;\propto
p({\bf w}_d|\beta_k)\frac{\alpha+c_{-d,k}}{\sum_{j=1}^K\alpha+c_{-d,j}},
\]
where now all the $z_d$ variables have become \Blue{dependent} (previously
they were conditionally independent given $\theta$).\\[1ex]

Notice, that the Gibbs sampler exhibits the \emph{rich get richer} property.
\end{frame}
\end{document}
