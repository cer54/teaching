<!DOCTYPE html>
<html>
<head>
<title>Probabilisitc Machine Learning (4F13) chunks</title>
</head>
<body>

<h2>Probabilistic Machine Learning chunks</h2>

</p>The topics and concepts taught in the Probabilisitc Machine Learning
course is broken down into a number of <em>chunks</em>, which are
detailed in this page. The goal of this organisation is to help
students to be able to identify and find material. Chunks are designed
to be concise, and fairly self contained, and clearly labeled with
content, prerequisites and relationships to other chunks.</p> 

<p>The entire course falls naturally in three parts, <a
href="#gp">Gaussian processes</a>, <a href="#rank">probabilistic
ranking</a> and <a href="#text">text modeling</a>. 


<h2><a name"gp"></a>Part I: Supervised non-parametric probabilistic
inference using Gaussian processes</h2>

<p>In a nutshell, part I is concerned with...</p>

<ol>
<li><a href="modelling data.pdf">Modelling data</a>
<ul>
  <li>goals of building a model</li>
  <li>requirements for good models</li>
  <li>data, parameters and latent variables</li>
</ul>
</li>

<li><a href="linear in the parameters regression.pdf">Linear in
  the parameters regression</a>
  <ul>
  <li>making predictions, concept of a model</li>
  <li>least squares fit, and the Normal equations</li>
  <li>requires linear algebra</li>
  <li>model complexity: underfitting and overfitting</li>
  </ul>
</li>

<li><a href="likelihood and noise.pdf">Likelihood and the concept of noise</a>
<ul>
  <li>Gaussian independent and identically distributed (iid) noise</li>
  <li>Maximum likelihood fitting</li>
  <li>Equivalence to least squares</li>
  <li>Motivation of inference with multiple hypothesis</li>
</ul>
</li>

<li><a href="probability fundamentals.pdf">Probability fundamentals</a>
<ul>
  <li>Medical example</li>
  <li>Joint, conditional and marginal probabilities</li>
  <li>The two rules of probability: sum and product</li>
  <li>Bayes' rule</li>
</ul>
</li>

<li><a href="bayesian finite regression.pdf">Bayesian inference and
prediction with finite regression models</a>
<ul>
  <li>Likelihood and prior</li>
  <li>Posterior and predictive distribution, pictorially</li>
  <li>Posterior algebra</li>
</ul>
</li>

<li>Background: <a href="gaussian and matrix equations.pdf">Some useful
Gaussian and Matrix equations</a>
<ul>
  <li>matrix inversion lemma</li>
  <li>mean and variance of Gaussian</li>
  <li>mean and variance of projection of Gaussian</li>
  <li>marginal and conditional of Gaussian</li>
  <li>products of Gaussians</li>
</ul>
</li> 

<li><a href="marginal likelihood.pdf">Marginal likelihood</a>
<ul>
  <li>Bayesian model selection</li>
  <li>MCMC based explanation of how the marginal likelihood works</li>
  <li>Average the likelihood over the prior: example with polynomials</li>
</ul>
</li>
    
<li><a href="parameters and functions.pdf">Distributions over
parameters and over functions</a>
<ul>
  <li>Motivation: representation of multiple hypotheses</li>
  <li>Concept of prior over functions and over parameters</li>
  <li>Inference</li>
  <li>Priors over functions are priors over long vectors</li>
</ul>
</li>

<li><a href="gaussian process.pdf">Gaussian process</a>
<ul>
  <li>From scalar Gaussians to multivariate Gaussians to Gaussian processes</li>
  <li>Marginal and conditional</li>
  <li>GP definition</li>
  <li>Conditional generation and joint generation</li>
</ul>
</li>

<li>Gaussian processes and data
<ul>
  <li>In pictures: prior and posterior</li>
  <li>In algebra: prior and posterior</li>
  <li>An analytic marginal likelihood, and some intuition</li>
  <li>Desirable properties of priors over functions (idea: write down
     the "telescopic" products of conditionals expansion, and realize
     that you cannot really do anything without marginalization)</li>
</ul>
</li>

<li>Linear in the parameters models are particular case GPs
<ul>
  <li>From infinite linear models to GPs: the Gaussian RBF case</li>
  <li>From infinite linear models to GPs: the infinite NN case (w/o proof)</li>
  <li>Splines are GPs</li>
  <li>What's your prior (polynomials are a bad idea)</li>
</ul>
</li>

<li>Gaussian process hyperparameters
<ul>
  <li>using hyper parameters to specify priors</li>
  <li>automatic relevance determination (ARD)</li>
  <li>how to fit hyperparameters</li>
</ul>
</li>

<li>Occam's Razor and Gaussian processes
<ul>
  <li>Occam's Razor and the marginal likelihood</li>
  <li>why doesn't the GP overfit</li>
</ul>
</li>

<li>covariance functions
<ul>
  <li>Stationary covariance functions, squared exponential, rational
  quadratic, Mat&eacute;rn covariance function</li>
  <li>polynomial covariance function</li>
  <li>periodic covariance</li>
  <li>neural network covariance function</li>
</ul>
</li>

<li>Practical model selection with GPs
<ul>
  <li>A variety of covariance functions and how to compose new ones
     (products, sums of cov funs, g(x)k(x,x')g(x'), etc)</li>
  <li>Evidence: optimize or integrate out?</li>
</ul>
</li>

<li>The gpml toolbox
</li>

</ol>

<h2><a name="rank"></a>Part II: Ranking</h2>

<ol start="17">
<li>Ranking: motivation and tennis example
<ul>
  <li>Competition in sports and games (TrueSkill problem, match making)</li>
  <li>Tennis: the ATP ranking system explained</li>
  <li>Shortcomings: what does one need to make actual predictions?
  (who wins?)</li>
</ul>
</li>

<li>The TrueSkill ranking model
<ul>
  <li>Change: get rid of variable 's' (difference of skills)</li>
  <li>Generative model with known skills</li>
  <li>The likelihood in pictures and equations</li>
  <li>Priors on skills</li>
  <li>Predictive distribution in closed analytical form</li>
  <li>Graphical representation wiht a factor graph for the generative model</li>
  <li>Question: in generative model factor graph the weights are not connected
      		to each other via common factors, however, the posterior over
		weights does not factorize. What?</li>
</ul>
</li>

<li>Factor graphs and graphical models
<ul>
  <li>Motivation</li>
  <li>Various types of graphical models</li>
  <li>Factor graphs to represent structured probabilistic models</li>
  <li>Message passing algorithms for inference</li>
</ul>
</li>

<li>Approximate inference with EP on the full graph
<ul>
  <li>Schedule on the full graph</li>
  <li>EP approximation</li>
  <li>Moments of truncated Gaussians</li>
  <li>A detailed program view in 6 steps</li>
</ul>
</li>

<li>Gibbs sampling
</li>

<li>Gibbs sampling inference applied to TrueSkill
<ul>
  <li>Gibbs sampling applied to True Skill, be super-explicit about
  sampling from p(w,t|y) and ignoring samples from t as a way to
  marginalize over it. Alternate between sampling from p(t|w,y) and
  p(w|t,y) = p(w|t).</li>
</ul>
</li>

</ol>

<h2><a name="text"></a>Part III: Modeling text</h2>

<ol start="33">
<li>Discrete distributions over binary variables (tossing coins)
<ul>
  <li>Philosophical discussion about coins and priors</li>
  <li>Distributions on probabilities</li>
  <li>Bernoulli, binomial, Beta</li>
  <li>Sequences of throws: difference between discrete distribution
       		    	    and Binomial distribution</li>
  <li>Bayes rule for discrete-Beta models</li>
</ul>
</li>

<li>Models of text
<ul>Problem setting: how to model documents?
  <li>Example: NIPS corpus</li>
  <li>Power laws on word frequencies...</li>
  <li>Word frequencies is the maximum likelihood solution</li>
</ul>
</li>

<li>A Bayesian model of text
<ul>
  <li>Posterior distributions over the probability simplex</li>
  <li>Dirichlet distributions</li>
</ul>
</li>

<li>Bayesian mixture of multinomials
</li>

<li><a href="expectation maximization.pdf">The Expectation
Maximization (EM) algoritm</a>
</li>

<li>LDA
</li>

</ol>

</body>
</html>
