<!DOCTYPE html>
<html>
<head>
<title>Probabilisitc Machine Learning (4F13) chunks</title>
</head>
<body>

<h2>Probabilistic Machine Learning chunks</h2>

</p>The topics and concepts taught in the Probabilisitc Machine Learning
course is broken down into a number of <em>chunks</em>, which are
detailed in this page. The goal of this organisation is to help
students to be able to identify and find material. Chunks are designed
to be concise, and fairly self contained, and clearly labeled with
content, prerequisites and relationships to other chunks.</p> 

<p>The entire course falls naturally in three parts, <a
href="#gp">Gaussian processes</a>, <a href="#rank">probabilistic
ranking</a> and <a href="#text">text modeling</a>. 


<h2><a name"gp"></a>Part I: Supervised non-parametric probabilistic
inference using Gaussian processes</h2>

<p>In a nutshell, part I is concerned with...</p>

<ol>
<li><a href="modelling data.pdf">Modelling data</a>
<ul>
  <li>goals of building a model</li>
  <li>requirements for good models</li>
  <li>data, parameters and latent variables</li>
</ul>
</li>

<li><a href="linear in the parameters regression.pdf">Linear in
  the parameters regression</a>
  <ul>
  <li>making predictions, concept of a model</li>
  <li>least squares fit, and the Normal equations</li>
  <li>requires linear algebra</li>
  <li>model complexity: underfitting and overfitting</li>
  </ul>
</li>

<li><a href="likelihood and noise.pdf">Likelihood and the concept of noise</a>
<ul>
  <li>Gaussian independent and identically distributed (iid) noise</li>
  <li>Maximum likelihood fitting</li>
  <li>Equivalence to least squares</li>
  <li>Motivation of inference with multiple hypothesis</li>
</ul>
</li>

<li><a href="probability fundamentals.pdf">Probability fundamentals</a>
<ul>
  <li>Medical example</li>
  <li>Joint, conditional and marginal probabilities</li>
  <li>The two rules of probability: sum and product</li>
  <li>Bayes' rule</li>
</ul>
</li>

<li><a href="bayesian finite regression.pdf">Bayesian inference and
prediction with finite regression models</a>
<ul>
  <li>Likelihood and prior</li>
  <li>Posterior and predictive distribution, with algebra and pictorially</li>
  <li>the marginal likelihood</li>
</ul>
</li>

<li>Background: <a href="gaussian and matrix equations.pdf">Some useful
Gaussian and Matrix equations</a>
<ul>
  <li>matrix inversion lemma</li>
  <li>mean and variance of Gaussian</li>
  <li>mean and variance of projection of Gaussian</li>
  <li>marginal and conditional of Gaussian</li>
  <li>products of Gaussians</li>
</ul>
</li> 

<li><a href="marginal likelihood.pdf">Marginal likelihood</a>
<ul>
  <li>Bayesian model selection</li>
  <li>MCMC based explanation of how the marginal likelihood works</li>
  <li>Average the likelihood over the prior: example</li>
</ul>
</li>
    
<li><a href="parameters and functions.pdf">Distributions over
parameters and over functions</a>
<ul>
  <li>Concept of prior over functions and over parameters</li>
  <li>nuissance parameters
  <li>Could we sidestep parameters, and work directly with functions?</li>
</ul>
</li>

<li><a href="gaussian process.pdf">Gaussian process</a>
<ul>
  <li>From scalar Gaussians to multivariate Gaussians to Gaussian processes</li>
  <li>Functions are like infinitely long vectors, GPs are
  distributions over functions</li>
  <li>Marginal and conditional Gaussian</li>
  <li>GP definition</li>
  <li>Conditional generation and joint generation</li>
</ul>
</li>

<li><a href="gp and data.pdf">Gaussian processes and data</a>
<ul>
  <li>In pictures: prior and posterior</li>
  <li>In algebra: prior and posterior</li>
  <li>An analytic marginal likelihood, and some intuition</li>
</ul>
</li>

<li><a href="hyperparameters.pdf">Gaussian process marginal likelihood
and hyperparameters</a>
<ul>
  <li>the GP marginal likelihood, and it interpretation</li>
  <li>hyperparameters can control the properties of functions</li>
  <li>example: finding hyperparameters by maximizing the marginal
  likelihood</li>
  <li>Occam's Razor</li>
</ul>
</li>  

<li><a href="correspondence.pdf">Correspondence between Linear in the
parameters models and GPs</a></li>
<ul>
  <li>From linear in the parameters models to GPs</li>
  <li>From GPs to linear in the parameters models</li>
  <li>Computational considerations: which is more efficient?</li>
</ul>
</li>

<li><a href="covariance functions.pdf">covariance functions</a>
<ul>
  <li>Stationary covariance functions, squared exponential, rational
  quadratic, Mat&eacute;rn covariance function</li>
  <li>periodic covariance</li>
  <li>neural network covariance function</li>
  <li>Combining simple covariance functions into more interesting ones</li>
</ul>
</li>

<li>The <a href="http://gaussianprocess.org/gpml/code">gpml toolbox</a>
</li>

</ol>

<h2><a name="rank"></a>Part II: Ranking</h2>

<ol start="15">
<li><a href="ranking.pdf">Ranking: motivation and tennis example</a>
<ul>
  <li>Competition in sports and games (TrueSkill problem, match making)</li>
  <li>Tennis: the ATP ranking system explained</li>
  <li>Shortcomings: what does one need to make actual predictions?
  (who wins?)</li>
  <li>The TrueSkill ranking model</li>
</ul>
</li>

<li><a href="gibbs sampling.pdf">Gibbs sampling</a>
<ul>
  <li>Calulating integrals using sampling</li>
  <li>Markov chains and invariant distributions</li>
  <li>Gibbs sampling</li>
</ul>
</li>

<li><a href="gibbs in TrueSkill.pdf">Gibbs sampling in TrueSkill</a>
<ul>
  <li>Conditional distributions in TrueSkill are tractable</li>
</ul>
</li>

<li>Representing distributions using <a href="factor graphs.pdf">
factor graphs</a>
<ul>
  <li>the cost of computing marginal distributions</li>
  <li>algebraic and graphical representations</li>
  <li>local computations on the graph</li>
  <li>message passing: the sum-product rules</li>
</ul>
</li>

<li><a href="message in TrueSkill.pdf">message passing in
TrueSkill</a>
<ul>
  <li>messages are not all tractable</li>
</ul>
</li>

<li>Approximate messages using <a href="moment matching.pdf">moment
matching</a>
<ul>
  <li>How to approximate a step function by a Gaussian?</li>
</ul>
</li>

<!--
<li>other issues
<ul>
  <li>Change: get rid of variable 's' (difference of skills)</li>
  <li>Generative model with known skills</li>
  <li>The likelihood in pictures and equations</li>
  <li>Priors on skills</li>
  <li>Predictive distribution in closed analytical form</li>
  <li>Graphical representation wiht a factor graph for the generative model</li>
  <li>Question: in generative model factor graph the weights are not connected
      		to each other via common factors, however, the posterior over
		weights does not factorize. What?</li>
</ul>
</li>

<li>Factor graphs and graphical models
<ul>
  <li>Motivation</li>
  <li>Various types of graphical models</li>
  <li>Factor graphs to represent structured probabilistic models</li>
  <li>Message passing algorithms for inference</li>
</ul>
</li>

<li>Approximate inference with EP on the full graph
<ul>
  <li>Schedule on the full graph</li>
  <li>EP approximation</li>
  <li>Moments of truncated Gaussians</li>
  <li>A detailed program view in 6 steps</li>
</ul>
</li>

<li>Gibbs sampling
</li>

<li>Gibbs sampling inference applied to TrueSkill
<ul>
  <li>Gibbs sampling applied to True Skill, be super-explicit about
  sampling from p(w,t|y) and ignoring samples from t as a way to
  marginalize over it. Alternate between sampling from p(t|w,y) and
  p(w|t,y) = p(w|t).</li>
</ul>
</li>
-->

</ol>

<h2><a name="text"></a>Part III: Modeling text</h2>

<ol start="33">
<li><a href="text.pdf">Modeling text</a></li>
<ul>
  <li>Modeling collections of documents</li>
  <li>probabilistic models of text</li>
  <li>Bag of words models</li>
  <li>Zipf's law</li>
</ul>

<li>Discrete distributions on <a href="binary.pdf">binary</a>
variables (tossing coins)
<ul>
  <li>Binary variables and the Bernoulli distribution</li>
  <li>Sequences, the binomial and discrete distributions</li>
  <li>Inference and the Beta distributinos: probabilities
  over probabilities</li>
</ul>
</li>

<li><a href="discrete.pdf">Discrete distributions</a> over multiple
outcomes</li>
<ul>
  <li>multinomials, categorical and discrete distributions</li>
  <li>inference and the Dirichlet prior</li>
</ul>
</li>

<li><a href="document models.pdf">Document models</a></li>
<ul>
  <li>Categorical model</li>
  <li>Mixture of categoricals model</li>
  <li>Trainig mixture models with EM</li>
  <li>A Bayesian mixture model</li>
</ul>
</li>

<li><a href="expectation maximization.pdf">The Expectation 
Maximization (EM) algoritm</a>
</li>
<ul>
  <li>Maximum likelihood in models with latent variables</li>
</ul>

<li><a href="gibbs for Bayesian mixture.pdf">Gibbs sampling for
Bayesian mixture model</a>
</li>
<ul>
  <li>Gibbs sampling</li>
  <li>Collapsed Gibbs sampling</li>
</ul>

<li><a href="lda.pdf">Latent Dichichlet Allocation</a> topic models</li>
<ul>
  <li>A more interesting topic model</li>
  <li>Inference using Gibbs sampling</li>
</ul>

</ol>

</body>
</html>
