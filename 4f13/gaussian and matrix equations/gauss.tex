\input{../style.tex}

\title[Gaussian Densities]{(Multivariate) Gaussian (Normal)\\ Probability Densities}
\author[Rasmussen, Hern\'andez-Lobato \& Turner]{Carl Edward
  Rasmussen,  Jos\'e Miguel Hern\'andez-Lobato \& Richard Turner}
\date{August 5th, 2020}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Gaussian Density}

The probability density of a $D$-dimensional Gaussian with mean vector
$\boldsymbol{\mu}$ and covariance matrix $\Sigma$ is given by
%
\[
p({\bf x}|\boldsymbol{\mu},\Sigma)\;=\;
{\cal N}({\bf x}|\boldsymbol{\mu},\Sigma)\;=\;\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}
\exp\big(\!-\tfrac{1}{2}({\bf x}-\boldsymbol{\mu})^\top \Sigma^{-1}
({\bf x}-\boldsymbol{\mu})\big),
\]
%
and we also write
%
\[
{\bf x}|\boldsymbol{\mu},\Sigma\;\sim\;{\cal N}({\bf x}|\boldsymbol{\mu},\Sigma).
\]
%
The covariance matrix $\Sigma$ must be symmetric and positive definite.\\[1ex]

In the special (scalar) case where $D=1$ we have
%
\[
p(x|\mu,\sigma^2)\;=\;\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\big(\!-\tfrac{1}{2}(x-\mu)^2/\sigma^2\big),
\]
%
where $\sigma^2$ is the variance and $\sigma$ is the standard deviation.\\[1ex]

The \emph{standard} Gaussian has $\boldsymbol{\mu}=\boldsymbol{0}$ and
$\Sigma=I$ (the unit matrix), shorthand
%
\[
{\cal N}({\bf x})\;=\;{\cal
  N}({\bf x}|\boldsymbol{\mu}=\boldsymbol{0},\Sigma=I).
\]

\end{frame}

\begin{frame}
\frametitle{Parametrisation}

There are two commonly used parametrisations of Gaussians
\begin{itemize}
\item \emph{standard} parametrisation:
\begin{itemize}
\item \emph{mean} $\boldsymbol{\mu}$ and
\item \emph{covariance} $\Sigma$
\end{itemize}
\item \emph{natural} parametrisation:
\begin{itemize}
\item \emph{natural mean}
  $\boldsymbol{\nu}=\Sigma^{-1}\boldsymbol{\mu}$ and
\item \emph{precision} matrix $R=\Sigma^{-1}$.
\end{itemize}
\end{itemize}
Different operations are more convenient in either parametrisation.

\end{frame}

\begin{frame}
\frametitle{Gaussian Pictures}

\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.4\textwidth]{{"../gaussian process/gauss00"}.pdf} &
{\includegraphics[width=0.4\textwidth]{{"../gaussian process/gauss01"}.pdf}}
\end{tabular}
\end{center}

The mean corresponds to the location or center of the
distribution.\\[1ex]

In one dimension, the square root of the variance corresponds to the
\emph{width} of the distribution.\\[1ex]

In multiple dimensions, the eigen-vectors of the covariance matrix
give the principal axis of the elliptical equi-probability contours of
the distribution, and the square root of the eigenvalues the width of the
distribution in the corresponding directions.
\end{frame}

\begin{frame}
\frametitle{Conditionals and Marginals of a Gaussian, pictorial}
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.45\textwidth]{{"../gaussian process/gauss02"}.pdf} &
{\includegraphics[width=0.45\textwidth]{{"../gaussian process/gauss03"}.pdf}}
\end{tabular}
\end{center}

Both the \Blue{conditionals $p(x|y)$} and the \Red{marginals $p(x)$}
of a joint Gaussian $p(x,y)$ are again Gaussian.
\end{frame}

\begin{frame}
\frametitle{Conditionals and Marginals of a Gaussian, algebra}

If ${\bf x}$ and ${\bf y}$ are jointly Gaussian
\[
p(\bfx,\bfy)\;=\;p\big(\Big[\!\begin{array}{c}\bfx\\ \bfy\end{array}
\!\Big]\big)\;=\;{\cal N}\big(\Big[\!\begin{array}{c}{\bf a} \\ {\bf b}\end{array}\!\Big],
\;\Big[\!\begin{array}{cc}A & B \\ B^\top & C\end{array}\!\Big]\big),
\]

we get the marginal distribution of ${\bf x}$, $p(\bfx)$ by
\[
p(\bfx,\bfy)\;=\;{\cal N}\big(\Big[\!\begin{array}{c}{\bf a} \\ {\bf b}
\end{array}\!\Big],
\;\Big[\!\begin{array}{cc}A & B \\ B^\top & C\end{array}\!\Big]\big)
\;\;\Longrightarrow\;\;p(\bfx)\;=\;{\cal N}({\bf a},\;A),
\]

and the conditional distribution of ${\bf x}$ given ${\bf y}$ by
\[
p(\bfx,\bfy)={\cal N}\big(\Big[\!\begin{array}{c}{\bf a} \\ {\bf b}
\end{array}\!\Big],
\;\Big[\!\begin{array}{cc}A & B \\ B^\top & C\end{array}\!\Big]\big)
\;\Longrightarrow\;p(\bfx|\bfy)={\cal N}({\bf a}+BC^{-1}(\bfy-\bfb),\;
A-BC^{-1}B^\top),
\]

where ${\bf x}$ and ${\bf y}$ can be scalars or vectors.
\end{frame}



\begin{frame}
\frametitle{Gaussian Properties}

Gaussians are closed both under marginalisation and conditioning.

If ${\bf x}$ and ${\bf y}$ are jointly Gaussian
\[
\left[\!\begin{array}{c}{\bf x}\\{\bf y}\end{array}\!\right] 
\]

\end{frame}

\begin{frame}
\frametitle{Kullback-Leibler Divergence (Relative Entropy)}

The Kullback-Leibler (KL) divergence between continuous distributions is
%
\[
{\cal KL}(q(x)||p(x))\;=\;\int q(x)\log\frac{q(x)}{p(x)}dx.
\]
%
The KL divergence is an asymmetric measure of distance between distributions.

The KL divergence between two Gaussians is
\[
{\cal KL}({\cal N}_0||{\cal N}_1)\;=\;\tfrac{1}{2}\log|\Sigma_1\Sigma_0^{-1}|
+\tfrac{1}{2}\operatorname{tr}\big(\Sigma_1^{-1}
\big((\boldsymbol{\mu}_0-\boldsymbol{\mu}_1)
(\boldsymbol{\mu}_0-\boldsymbol{\mu}_1)^\top + \Sigma_0 - \Sigma_1\big)\big).
\]

\end{frame}

\begin{frame}
\frametitle{KL matching constrained Gaussians}

It is often convenient to approximate one distribution with another,
simpler one, by finding the \emph{closest match} within a constrained family.\\[1ex]

Minimizing KL divergence between a \Green{general Gaussian ${\cal N}_g$} and a
factorized Gaussian ${\cal N}_f$ will match the means
$\boldsymbol\mu_f=\boldsymbol\mu_g$ and for the covariances either:
\[
\frac{\partial {\KL}(\Red{{\cal N}_f}||{\cal N}_g)}{\partial\Sigma_f}=
-\tfrac{1}{2}\Sigma_f^{-1}+\tfrac{1}{2}\Sigma_g^{-1}=0\;\Rightarrow\;
\Red{(\Sigma_f)_{ii} = 1/(\Sigma_g^{-1})_{ii}},
\]
or
\[
\frac{\partial {\KL}(\Blue{{\cal N}_g}||{\cal N}_f)}{\partial\Sigma_f}=
\tfrac{1}{2}\Sigma_f^{-1}-\tfrac{1}{2}\Sigma_f^{-1}\Sigma_g\Sigma_f^{-1}=0\;
\Rightarrow\;\Blue{(\Sigma_f)_{ii} = (\Sigma_g)_{ii}}.
\]
%
\parbox{0.7\linewidth}{
Interpretation:
\begin{itemize}
\item \Red{averaging wrt the \emph{factorized} Gaussian}, the fitted variance
equals the \Red{\emph{conditional} variance} of $\Green{\Sigma_g}$,
\item \Blue{averaging wrt the \emph{general} Gaussian}, the fitted variance
equals the \Blue{\emph{marginal} variance} of $\Green{\Sigma_g}$,
\end{itemize}
with straight forward generalization to block diagonal Gaussians.
}
\parbox{0.29\linewidth}{
\hfill
\includegraphics[width=\linewidth]{kl}
}

\end{frame}

\begin{frame}
\frametitle{Incomplete (truncated) scalar Gaussian integrals}

Let $\Phi(z)$ be the standard cumulative Gaussian
%
\[
\Phi(z)\;=\;\int_{-\infty}^z{\cal N}(x)dx\;=\;
\int_{-\infty}^z\frac{1}{\sqrt{2\pi}}
\exp(-\tfrac{1}{2}x^2)dx.
\]
%
We then have the following incomplete Gaussian integrals
%
\[
\int_a^b{\cal N}(x|\mu,\sigma^2)dx\;=\;\Phi(\beta)-\Phi(\alpha),
\text{\ \ where\ \ }
\alpha\;=\;\frac{a-\mu}{\sigma}\text{\ \ and\ \ }
\beta\;=\;\frac{b-\mu}{\sigma}.
\]
%
Further
%
\[
  \int_a^b\frac{x-\mu}{\sigma}{\cal N}(x|\mu,\sigma^2)dx\;=\;
  {\cal N}(\alpha)-{\cal N}(\beta),
\]
%
and
%
\[
 \int_a^b\Big(\frac{x-\mu}{\sigma}\Big)^2{\cal N}(x|\mu,\sigma^2)dx\;=\;
  \alpha{\cal N}(\alpha)-\beta{\cal
    N}(\beta)+\Phi(\beta)-\Phi(\alpha),
\]
%
which can both be shown by integration by parts. Both expressions have
the expected behaviour as $a\rightarrow-\infty$ and/or
$b\rightarrow\infty$ (one sided Gaussians).

\end{frame}


\begin{frame}

\frametitle{Appendix: Some useful Gaussian identities}
If $x$ is multivariate Gaussian with mean $\mu$ and
covariance matrix $\Sigma$
\[
p({\bf x}; \mu, \Sigma)\;=\;(2\pi|\Sigma|)^{-D/2}\exp\big(-({\bf
    x}-\mu)^\top\Sigma^{-1}({\bf x}-\mu)/2\big),
\]
then
\[
\begin{split}
\E[{\bf x}]\;&=\;\mu,\\
\V[{\bf x}]\;&=\;\E[({\bf x}-\E[{\bf x}])^2]\;=\;\Sigma.
\end{split}
\]

For any matrix $A$, if ${\bf z} = A{\bf x}+{\bf b}$ then ${\bf z}$ is Gaussian and
\[
\begin{split}
\E[{\bf z}] \;&=\; A\mu+{\bf b},\\
\V[{\bf z}] \;&=\;A\Sigma A^\top.
\end{split}
\]
\end{frame}

\begin{frame}
\frametitle{Matrix and Gaussian identities cheat sheet}

Matrix identities
\begin{itemize}
\item Matrix inversion lemma (Woodbury, Sherman \& Morrison formula)
%
\[
(Z+UWV^\top)^{-1}=Z^{-1}-Z^{-1}U(W^{-1}+V^\top Z^{-1}U)^{-1}V^\top Z^{-1}
\]
%
\item A similar equation exists for determinants
\[
|Z+UWV^\top|=|Z|\;|W|\;|W^{-1}+V^\top Z^{-1}U|
\]
\end{itemize}

The product of two Gaussian density functions
%
\[
\N(\bfx|{\mathbf a},A)\,\N(P^\top\,\bfx|{\mathbf b},B) = z_c\,
\N(\bfx|{\mathbf c},C)
\]
%
\vspace{-5mm}
\begin{itemize}
\item is proportional to a Gaussian density function with covariance and mean
%
\[
C = \left(A^{-1}+P\,B^{-1}P^\top\right)^{-1}\enspace \hspace{1cm} c =
C\,\left(A^{-1}{\mathbf a}+P\,B^{-1}\,{\mathbf b}\right)
\]
%
\item and has a normalizing constant $z_c$ that is Gaussian both in ${\mathbf
a}$ and in ${\mathbf b}$ 
%
\[
z_c = (2\,\pi)^{-\frac{m}{2}}|B+P^\top A\,P|^{-\frac{1}{2}}
\exp\big(-\frac{1}{2}({\mathbf b}-P^\top\,{\mathbf a})^\top
\left(B+P^\top A\,P\right)^{-1}({\mathbf b}-P^\top\,{\mathbf a})
\big)
\]
%
\end{itemize}
\end{frame}

\end{document}

