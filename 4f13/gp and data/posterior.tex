\input{../style.tex}

\title{Posterior Gaussian Process}
\author{Carl Edward Rasmussen}
\date{October 17th, 2022}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Key concepts}
\begin{itemize}
\item we are not interested in random functions
\item we want to \emph{condition} on the training data
\item when both prior and likelihood are Gaussian, then
\begin{itemize}
\item posterior is a Gaussian process
\item predictive distributions are Gaussian
\end{itemize}
\item pictorial representation of prior and posterior
\item interpretation of predictive equations
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Gaussian Process Inference}

Recall Bayesian inference in a parametric model.\\[1ex]

The posterior is proportional to the prior times the likelihood.\\[1ex]

The predictive distribution is the predictions marginalized over the
parameters.\\[1ex]

How does this work in a Gaussian Process model?\\[1ex]

Answer: in our non-parametric model, the ``parameters'' are the function itself!
\end{frame}


\begin{frame}
\frametitle{Non-parametric Gaussian process models}

In our non-parametric model, the ``parameters'' are the function itself!

The joint distribution
\[
  \begin{split}
    p(f,\bfy)\;&=\;\Blue{p(f)}\,\Red{p(\bfy|\bff)}\;=\;p(\bfy)\Green{p(f|\bfy)}\\
    &\Longrightarrow\;\Blue{{\cal N}(f|m,\;k)}\,\Red{{\cal
        N}(\bfy|\bff)}\;=\;
    Z_{|\bfy}\Green{{\cal N}(f|m_{|\bfy},k_{|\bfy})}.
    \end{split}
\]

Gaussian process prior with zero mean and covariance function $k$
\[
\Blue{p(f|\mathcal{M}_i)\;\sim\;{\cal N}(f|m\equiv 0,\;k),}
\]

Gaussian likelihood, with noise variance $\sigma_{\rm noise}^2$
\[
\Red{p(\bfy|\bff, \mathcal{M}_i)\;\sim\;
{\cal N}(\bff,\;\sigma^2_{\rm noise}I),}
\]


leads to a Gaussian process posterior
\[
\begin{split}
\Green{p(f}&\Green{|\bfy, \mathcal{M}_i)\;\sim\;{\cal
  N}(f|m_{|\bfy},\;k_{|\bfy}),}\\
&\text{where}\left\{\!\!\begin{array}{l}\Green{m_{|\bfy}(x)=k(x,\bfx)[K(\bfx,\bfx)+\sigma_{\rm noise}^2I]^{-1}\bfy,}\\
\Green{k_{|\bfy}(x,x')=k(x,x')-k(x,\bfx)[K(\bfx,\bfx)+\sigma^2_{\rm noise}I]^{-1}k(\bfx,x').}\end{array}\right.
\end{split}
\]
\end{frame}

\begin{frame}
\frametitle{Prior and Posterior}

\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.45\textwidth]{priorpost} &
{\includegraphics[width=0.45\textwidth]{priorpost1}}
\end{tabular}
\end{center}

Predictive distribution:
\[
\begin{split}
p(y_*|x_*,\bfx,\bfy)\;\sim\;{\cal N}\big(&\bfk(x_*,\bfx)^\top
[K+\sigma_{\rm noise}^2I]^{-1}\bfy,\\
&k(x_*,x_*)+\sigma_{\rm noise}^2 - \bfk(x_*,\bfx)^\top 
[K+\sigma_{\rm noise}^2I]^{-1}\bfk(x_*,\bfx)\big)
\end{split}
\]
\end{frame}

\begin{frame}
\frametitle{Some interpretation}

Recall our main result:
\[
\begin{split}
f_*|x_*,\bfx,{\bf y}\;\sim\; {\cal N}\big(&\Blue{K(x_*,\bfx)
[K(\bfx,\bfx)+\sigma_\mathrm{noise}^2I]^{-1}{\bf y},}\\
&\Red{K(x_*,x_*)-K(x_*,\bfx)[K(\bfx,\bfx)+\sigma_\mathrm{noise}^2I]^{-1}K(\bfx,x_*)}\big).
\end{split}
\]
The \Blue{mean} is linear in two ways:
\[
\mu(x_*)\;=\;\Blue{k(x_*,\bfx)[K(\bfx,\bfx)+\sigma_\mathrm{noise}^2I]^{-1}\bfy}
\;=\;\sum_{n=1}^N\beta_n y_n\;=\;
\sum_{n=1}^N\alpha_nk(x_*,x_n).
\]
The last form is most commonly encountered in the kernel literature.

The \Red{variance} is the difference between two terms:
\[
V(x_*)\;=\;\Red{k(x_*,x_*)-{\bf k}(x_*,\bfx)
[K(\bfx,\bfx)+\sigma_\mathrm{noise}^2I]^{-1}{\bf k}(\bfx,x_*)},
\]
the first term is the \emph{prior variance}, from which we subtract
a (positive) term, telling how much the data $\bfx$ has
explained.\\
Note, that the variance is independent of the observed
outputs ${\bf y}$.
\end{frame}


\end{document}
