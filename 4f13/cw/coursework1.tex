\documentclass[12pt]{article}
\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX
\setmainfont{Sabon LT Std} % text font Sabon
\usepackage[OT1,euler-digits]{eulervm} % math font euler
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{amsmath}

\parskip=2ex
\parindent=0ex
\textheight=270mm
\textwidth=7in
\oddsidemargin=-0.25in
\topmargin=-25mm
\pagestyle{empty}

\pagestyle{empty}
\newcommand{\bi}{\begin{itemize}\vspace*{-0.3in}}
\newcommand{\ei}{\end{itemize}\vspace*{-0.3in}}
\newcommand{\im}{\item\vspace*{-0.4in}}
\newcommand{\new}{^{\scriptscriptstyle \mathrm{new}}}
\newcommand{\bea}{\begin{eqnarray*}}
\newcommand{\eea}{\end{eqnarray*}}
\newcommand{\T}{{\scriptsize^{\top}}}
\newcommand{\yy}{{\mathbf y}}
\newcommand{\by}{{\mathbf y}}
\newcommand{\xx}{{\mathbf x}}
\newcommand{\bx}{{\mathbf x}}
\newcommand{\zz}{{\mathbf z}}
\newcommand{\Bet}{{\rm Beta}}
\newcommand{\deldel}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\hh}{\hspace{0.2in}}
\newcommand{\bpi}{{\boldsymbol{\pi}}}
\newcommand{\bmu}{{\boldsymbol{\mu}}}
\newcommand{\btheta}{{\boldsymbol{\theta}}} % amssymb
\newcommand{\blambda}{{\boldsymbol{\lambda}}} % amssymb
\newcommand{\bs}{{\mathbf s}}
\newcommand{\be}{\begin{equation*}}
\newcommand{\ee}{\end{equation*}}

\begin{document}

\centerline{\large\bf 4F13 Probabilistic Machine Learning: Coursework \#1: Gaussian Processes}

\vspace{0.3cm}

\centerline{Carl Edward Rasmussen} 

\vspace{0.2cm}

\centerline{Due (for non-MLMI students) 12:00 noon, Friday Nov 8th, 2024 online via moodle}

\vspace{2mm}

Your answers should contain an explanation of what you do, and max 2-4
central commands to achieve it (but complete listings are unnecessary
and discouraged). You must give an \emph{interpretation} of what the
numerical values and graphs you provide \emph{mean} -- why are the
results the way they are, and what are the consequences? Explain your
reasoning. {\bf Each question should be labelled and answered
  separately and distinctly.} Total combined length of answers must
not exceed 5 sides of a4 (plus cover page), minimum 11pt font, 1 inch
margins.

You need the Gaussian Processes for Machine Learning (GPML) toolbox
(version 4.2) for matlab and octave. Get the toolbox and walk through
the documentation concerning regression from the Gaussian Process Web
site at \texttt{www.gaussianprocess.org/gpml/code}\ \ Note, that
sometimes hyperparameters are encoded using their logarithms (to avoid
having to deal with \emph{constrained} optimization for positive
parameters), but you will want to report them in their natural
domain. All logs are natural (ie, base $e$). All questions carry
approximately equal weight.
%
\begin{enumerate}

\item[a)] Load data from \texttt{cw1a.mat}. Train a GP with a
  squared exponential covariance function, \texttt{covSEiso}. Start
  the log hyper-parameters at \texttt{hyp.cov = [-1 0]; hyp.lik = 0;} and
  minimize the negative log marginal likelihood. Show the 95\%
  predictive error bars. Explain the values of the optimized
  hyperparameters and the shape of the predictive error bars.

\item[b)] How can you find out whether the hyper parameter optimum is
  unique, or whether there may be other local optima? If there are
  local optima, find some, and explain what the model is doing in each
  case. Which fit is best, and why? Quantify how confident are you about this and why?

\item[c)] Train instead a GP with a periodic covariance
  function, \texttt{covPeriodic}. Compare the behaviour of the error-bars with a). Do you
  think the data generating mechanism (apart from the noise) was
  strictly periodic\footnote{By strictly periodic, is meant a
    function where the exists a $p$ such that $f(x)=f(x+np)$ for
    integer $n$ and all
    $x$, not just a a function which ``goes up and down''.}? Carefully
  discuss the evindence for or against periodicity.  
  
\item[d)] Generate random (essentially) noise free functions evaluated
  at \texttt{x = linspace(-5,5,200)';} from a GP with the following
  covariance function: \texttt{\{@covProd, \{@covPeriodic,
    @covSEiso\}\}}, with covariance log hyperparameters
  \texttt{hyp.cov = [-0.5 0 0 2 0]}. In order to apply the Cholesky
  decomposition to the covariance matrix, you may have to add a small
  diagonal matrix, for example \texttt{1e-6*eye(200)}, why?  Plot some
  sample functions. Carefully explain the relationship between the
  properties of those random functions and the form of the covariance
  function.

\item[e)] Load \texttt{cw1e.mat}. This data has 2-D input and
  scalar output. Visualise the data, for example using
  \texttt{mesh(reshape(x(:,1),11,11),reshape(x(:,2),11,11),reshape(y,11,11));}
  Rotate the data, to get a good feel for it. Compare two GP models of
  the data, one with \texttt{covSEard} covariance and the other with 
  \texttt{\{@covSum, \{@covSEard, @covSEard\}\}} covariance. For the
  second model be sure to break symmetry with the initial
  hyperparameters (eg by using \texttt{hyp.cov =
    0.1*randn(6,1);}). 

  Compare the models: give a careful quantitative interpretation of
  the relationship between data fit, model complexity and marginal
  likelihood for each of the two models; which model is best and why,
  explain your reasoning.

\end{enumerate}

\end{document}
