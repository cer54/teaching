= Improvements =

* Finite linear in the parameters probabilistic inference part:
- Be explicit about penalized ML, aka maximum a posteriori.

* GP part:
- Be more careful about "sampling functions." It's possible
to sample a finite collection of function values, and one can
condition on those to obtain a narrower posterior. But it's important
to realize that there exist an infinite number of functions that share
this common subset of function values.

* Tennis exercise: subtract 0.5 instead of 1 from PP matrix.

* How do the exercises fit in the coursera view of the course?
